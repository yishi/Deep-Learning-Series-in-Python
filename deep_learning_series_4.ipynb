{
 "metadata": {
  "name": "",
  "signature": "sha256:b02ce232f8d84aa578168f74a1baa8a1cfcfebca03b660dc1ee3c91dffd43baa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Deep Learning Series 4: Stacked Denoising Autoencoders (SdA)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from IPython.display import Image"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 1. The Introduction of Autoencoders\n",
      "\n",
      "An autoencoder takes an input **x** and then first maps it (with an **encoder**) to a hidden representation **y** through a derterministic mapping, e.g.:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(\"autoencoder_1.PNG\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAIsAAAAkCAYAAAC5U8nEAAAAAXNSR0IArs4c6QAAAARnQU1BAACx\njwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAOeSURBVHhe7ZrRjepADEX54JNvCqACSqAEaqAG\nuqAIyqAK6qCMPJ2VjPy8kxl7kuyuiCNFLBnPxHN9fe2E3Qx5JAJOBDZOuzRLBIYkS5LAjUCSxQ1V\nGiZZkgNuBJIsbqjSMMmSHHAjkGRxQ5WGSZbkgBuBJIsbqjRMsiQH3Aisgiyv12u4XC5uUNZg2IPH\nKshyOp3WEP/wHqO4fDxZzufz8Hg83kCSUfv9fthsNl/ndrv9+o76cPI312Sc7zoLb7fbe/5utxui\ngIcj6pxwv9/ffuOz52BORGE+miwE/3A4FHETwpTGAVHIoommF/IGxBO0kg0+PJ/P0HT2i98R3yRR\nPDf6aLKQNdfrtYgDCiGEKAVF1OV4PH6bTyCXVhT85j6Ro4csNYzsvT+aLGRNLTuFEJQqfTBHiMQn\nQdAHBIpmfSTo2P4UWVDOMfXtJgvOU6MFRA0w7OQ649FsiIJo7fFLnzqIkKF2oA7St2g7rmvl0epU\nKm2M6z5I+gApZ7ov8u53ClmIA/FhH5z4xqclvfjSwknsQsoiMlfKtkid9ALWsrMNGoBIoCANoNUO\nrSCQQw7ZCwoiSSBjrK9treJgr32I9AR6rSlkwQfdaxE3CDGGB2NjRNI+hcjCRBiLM7pm1wC0GUsA\nPKctDaWgS+bImN4wYHkIbBtdiGCVQYPfWlPWE7XpLVdTyFJSClHR0tMPJPL4GSZLSV1aALYUondc\nP7VQd3U2ecliG13W0aST0gvBWbPV2EoWQ7Cx5trulzVtAoEpp71eSyKJTYkskuSlhn0xslh18apK\nLyFa8wCIgEvJEHtPGdI1m+BCFAum9GMy7slA1pFeyCPvpT3OrSw1sixWhtiYVpeIqpQyaKwktcoQ\n97WZazPK27gJkATYNujRvUrfJIoVwWeunqVWhkpq58UpXIZkQwKwV2pbChEdt+WiVCJaj85ahWwj\nW2pcxxpb3fxqkteyubXfKcoy1uCWiItSzv7obDcXkfkWMD3jKIB+ZC41bpEXTgA29uobItayD1Lo\n3sYmlLxVjSTWFLIQG/voPKbUEYzcyoIDGszITXrIMMec2uv+OdZfcg1U7CfeWVmFru3JTRaRVDYR\nka4lAfWsbX9I9MxZi81iPyRK2ZF3G71d/m8EovW4+xs+/YV7RnFxK8tf2FyvD/nPT9+Ro42IJvwq\nyNJLspz3PwJJlmSEG4EkixuqNEyyJAfcCCRZ3FClYZIlOeBGIMnihioN/wG73HHqfoi/+QAAAABJ\nRU5ErkJggg==\n",
       "prompt_number": 2,
       "text": [
        "<IPython.core.display.Image at 0x3f4ae80>"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Where **s** is a non-linearity such as the **sigmoid**. \n",
      "\n",
      "The latent representation **y**, or code is then mapped back (with a **decoder**) into a reconstruction **z** of the same shape as x. The mapping happens through a similar transformation, e.g.:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(\"autoencoder_2.PNG\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAJAAAAArCAYAAACXSwEOAAAAAXNSR0IArs4c6QAAAARnQU1BAACx\njwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAQESURBVHhe7ZrdbeswDEbz0Mc+Z4BM0BE6QmfI\nDNkiQ2SMTpE5OoYvTgEWBCFLVCgnuBYNGG1iSxY/Hv7I7WHJIxUIKHAIjM2hqcCSACUEIQUSoJB8\nOTgBSgZCCiRAIflycAKUDIQUSIBC8uXgBCgZCCmQAIXky8EJUDIQUiABCsmXgxOgAAOn02m53++B\nGV4/NGrD7gD6+flZzufz5p75/v5eDofDfw2QteER3XYH0Ofn5+bw8ICvr6/l4+PjKc/a6iElG3r1\n2xVACEJUyUFEHY/H30zB+fb29vuZLMXJ73wn1/mso/B6vf6Nf39/X7S4fNbP2srJPfPebrc/e7Cl\ndZRsYI6eTLQbgACCel46BKLSdQQTgNaAsM4Q+FoOil5nbb09FmvDnhZANRskyDzr3w1ARM3lcina\nTCYRSEoOkSxUKkk40aZ1nsOcWx88h+f3HF6AajbUtLRr2Q1ARE0tWgUSypw+GCNw8RMH6AOo7Ly9\nTu0BQN+7JUA1G8jEa9m8GyDpHRCSSGRiEdyK/ahQ3nEIqk/tWACpHaxdbNH38b3OUDqL1cqidTT9\nhOiiISWa+Z7rveBFAOJ5rAP7OAkwfnp91tJT7G9mICbSDxWh1sqFF4be+2xzhxjS7AES66odOtPo\n8iO9AgEijpZ5mN9bqqR0lLJYqx9ZW3cEINahezrWhy9bOslarN/X1lgFyEagiOzZvuJg7vOctqyU\nFiuRJNc01AjlcZJtpoFDICw105459VqxA8fpnqkHQmt3BKBSBpEs7NllAZqngW9mIHEUxtgI7c0i\nkfu1gymjOrq8ANlmmnlK2RXombP3nUgpC3khLAUcYzltENYCTtZQAkgA9ySAYQDhdHljqd+84gwP\noRFo7FjE4bmSCeW6p4Tp1IwdwGOFlH5Frj9in85CkezDekdnoB6AhpQwEb3U95R2J7Y59ZQv7mmV\nMKLQ9lw2wrxNn4gIJLap1RnEmzlKkOsXk5EAGg2QlDBP/+rVs1nCSn2PNKTejj4iImNtqSmVl9Y2\nXmerWikWe73Nc8k2gdTjqJo2EYDWmmhPYODfIdt43TNIytfb+CgY3vFkCr19LzWBPS+/sGGtkQRO\nb/Strb+npG4FEGuw2/hWpte7Ty/8zQzkdfKr7/O+s9linThLA9kDc209BHDvu6MR9tmMX5tzNwBh\npP1j6ggxPXNIyZKNhTf9e+Z+9j3T/jFVhO7deo9wkJQseVf1rN5wxNrtHL367SoDIcaz/qFsC+e9\nek5Kby/8uwPo1U6Y7fkJ0GweH2xvAjRY0NmmS4Bm8/hgexOgwYLONl0CNJvHB9ubAA0WdLbpEqDZ\nPD7Y3gRosKCzTZcAzebxwfYmQIMFnW26BGg2jw+29x/+e8c3QVuE3gAAAABJRU5ErkJggg==\n",
       "prompt_number": 3,
       "text": [
        "<IPython.core.display.Image at 0x3fa9198>"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parameters of this model (namely W, b) are optimized such that the average reconstruction error is minimized.\n",
      "\n",
      "The reconstruction error can be measured in many ways, depending on the appropriate distribution assumptions on the input given the code.\n",
      "\n",
      "The traditional **squared error** can be used.\n",
      "\n",
      "If the input is interpreted as either bit vectors or vectors of bit probabilities, **cross-entropy** of the reconstruction can be used."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(\"autoencoder_3.PNG\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAI8AAAAZCAYAAAAIXH3NAAAAAXNSR0IArs4c6QAAAARnQU1BAACx\njwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAAAM+SURBVGhD7ZrNseowDIVZsGRNAVRACZRADdRA\nFxRBGVRBHZSRNx8zYvx8bUv+TRbOTObOTWRL5/hEkhN2yzwmA4UM7ArHzWGTgWWKZ4qgmIEpnmLq\n5sApnqkBlYHH47Hcbrfler0ul8vlZ99UPJ/P5+uk19FzbkvM7/f7j1nommWurdhomLh/v99/4SIe\nRMTRVDyuKnuRM8JHLPbT6fTnVuhaL+w95tUwkXUOh8PPNf8fj8e0eMgiKIyBu93um65EcSEQ3Hu9\nXj3w/Tfn8/nsmt1SADSiu4Pv4MCCyV1Xsv/5fLZlHlQmxrHYEdrIJ5CY8Dn6sBA9OqZaf7mYSCZS\n6tSyRdZxa14oWNSo2dSCdMeP9ie+c4luibnXXDmYqD5kfjmS4sEQ8WhNIZlAs2kJnjQ6MtNN8Szf\n5CDly5R56GP2+7267iEbnCEqxMcpOyURJGPc8oMN1yiRKBxxyNhQibLEpQaeaaA9pbmYM91nmVv5\n1DDhlCaZk3VAOLKWyczD4mq7GyZzu3EfIWJwxYN9qGdBDK5IpFGPlUPfPovZQmML0UxtxVwYhmmY\nlU8NE9lGHmL5q27VWUiMUVzqYHLZusXsJAPJk+mXOL/hFvJTjbrbuMX8InzmsJypnWRJ2dIwmxRQ\naJTDpyaeVAjRzINoEE9sVyP1zyIe5uBJSDXf4geBYZfKZgCyiKeQ++iwHKItmH1H0svhRzu1F6ZW\nPnMw+fFGxcNTG1tAQLrNk7bQOJUeJlVu3BQp2QkRh5rxLZctN1NJ77HGqwULn13EgyBi/Y5/XWte\nsedJkWwWK3OhPoeSExOPllnWLFtWzBqGmvsWPpuLJ9Xv0Bv4KTO1VZcPakIC43ka/X4m1OcgmlDp\n5PqWt+pWzDXC0MZa+WwqHhZXmj35JCFfU6Vv8VNw6KWd+2nDFYqIB1Hghx5HMhLXAIO9u1X3idrq\nS8IczNri19zP4bOpeEqCHv15AsBr9BA1RJfwOmJMDSb184QVwPwwamVqW3abEA+UaC8UW9A2wkcs\nzhqiW2DvMUcNpmaZB2Ajfgy2Rrlyt97+Aq7RuLcU0WbE0xLUFucKvfHWfq6yRRxuTDWYmmaerRM1\n42vLwD++1vefbpSH5AAAAABJRU5ErkJggg==\n",
       "prompt_number": 4,
       "text": [
        "<IPython.core.display.Image at 0x3fa92b0>"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Image(\"autoencoder_4.PNG\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAABDCAYAAACGG5LlAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\nBRCAAASeSeAfh2vHcd49saEAAAAASUVORK5CYII=\n",
       "prompt_number": 5,
       "text": [
        "<IPython.core.display.Image at 0x3fa9390>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 2. The Introduction of Denoising Autoencoders\n",
      "\n",
      "The idea behind denoising autoencoders is simple. \n",
      "\n",
      "In order to force the hidden layer to discover more robust features and prevent it from simply learning the identity, we train the autoencoder to **reconstruct the input from a corrupted version of it**.\n",
      "\n",
      "The denoising auto-encoder is a stochastic version of the auto-encoder. \n",
      "\n",
      "Intuitively, a denoising auto-encoder does two things: \n",
      "\n",
      "- try to encode the input (preserve the information about the input)\n",
      "- try to undo the effect of a corruption process stochastically applied to the input of the auto-encoder\n",
      "- the latter can only be done by capturing the statistical dependencies between the inputs\n",
      "\n",
      "To convert the autoencoder class into a denoising autoencoder class, all we need to do is to add a stochastic corruption step operating on the input. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 3. Code about dA class\n",
      "\n",
      "We want to implement an denoising autoencoder using Theano, in the form of a class, that could be afterwards used in constructing a stacked autoencoder."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class dA(object):\n",
      "    \"\"\"Denoising Auto-Encoder class (dA)\n",
      "\n",
      "    A denoising autoencoders tries to reconstruct the input from a corrupted\n",
      "    version of it by projecting it first in a latent space and reprojecting\n",
      "    it afterwards back in the input space. Please refer to Vincent et al.,2008\n",
      "    for more details. If x is the input then equation (1) computes a partially\n",
      "    destroyed version of x by means of a stochastic mapping q_D. Equation (2)\n",
      "    computes the projection of the input into the latent space. Equation (3)\n",
      "    computes the reconstruction of the input, while equation (4) computes the\n",
      "    reconstruction error.\n",
      "\n",
      "    .. math::\n",
      "\n",
      "        \\tilde{x} ~ q_D(\\tilde{x}|x)                                     (1)\n",
      "\n",
      "        y = s(W \\tilde{x} + b)                                           (2)\n",
      "\n",
      "        x = s(W' y  + b')                                                (3)\n",
      "\n",
      "        L(x,z) = -sum_{k=1}^d [x_k \\log z_k + (1-x_k) \\log( 1-z_k)]      (4)\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        numpy_rng,\n",
      "        theano_rng=None,\n",
      "        input=None,\n",
      "        n_visible=784,\n",
      "        n_hidden=500,\n",
      "        W=None,\n",
      "        bhid=None,\n",
      "        bvis=None\n",
      "    ):\n",
      "        \"\"\"\n",
      "        Initialize the dA class by specifying the number of visible units (the\n",
      "        dimension d of the input ), the number of hidden units ( the dimension\n",
      "        d' of the latent or hidden space ) and the corruption level. The\n",
      "        constructor also receives symbolic variables for the input, weights and\n",
      "        bias. Such a symbolic variables are useful when, for example the input\n",
      "        is the result of some computations, or when weights are shared between\n",
      "        the dA and an MLP layer. When dealing with SdAs this always happens,\n",
      "        the dA on layer 2 gets as input the output of the dA on layer 1,\n",
      "        and the weights of the dA are used in the second stage of training\n",
      "        to construct an MLP.\n",
      "\n",
      "        :type numpy_rng: numpy.random.RandomState\n",
      "        :param numpy_rng: number random generator used to generate weights\n",
      "\n",
      "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
      "        :param theano_rng: Theano random generator; if None is given one is\n",
      "                     generated based on a seed drawn from `rng`\n",
      "\n",
      "        :type input: theano.tensor.TensorType\n",
      "        :param input: a symbolic description of the input or None for\n",
      "                      standalone dA\n",
      "\n",
      "        :type n_visible: int\n",
      "        :param n_visible: number of visible units\n",
      "\n",
      "        :type n_hidden: int\n",
      "        :param n_hidden:  number of hidden units\n",
      "\n",
      "        :type W: theano.tensor.TensorType\n",
      "        :param W: Theano variable pointing to a set of weights that should be\n",
      "                  shared belong the dA and another architecture; if dA should\n",
      "                  be standalone set this to None\n",
      "\n",
      "        :type bhid: theano.tensor.TensorType\n",
      "        :param bhid: Theano variable pointing to a set of biases values (for\n",
      "                     hidden units) that should be shared belong dA and another\n",
      "                     architecture; if dA should be standalone set this to None\n",
      "\n",
      "        :type bvis: theano.tensor.TensorType\n",
      "        :param bvis: Theano variable pointing to a set of biases values (for\n",
      "                     visible units) that should be shared belong dA and another\n",
      "                     architecture; if dA should be standalone set this to None\n",
      "\n",
      "\n",
      "        \"\"\"\n",
      "        self.n_visible = n_visible\n",
      "        self.n_hidden = n_hidden\n",
      "\n",
      "        # create a Theano random generator that gives symbolic random values\n",
      "        if not theano_rng:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "\n",
      "        # note : W' was written as `W_prime` and b' as `b_prime`\n",
      "        if not W:\n",
      "            # W is initialized with `initial_W` which is uniformely sampled\n",
      "            # from -4*sqrt(6./(n_visible+n_hidden)) and\n",
      "            # 4*sqrt(6./(n_hidden+n_visible))the output of uniform if\n",
      "            # converted using asarray to dtype\n",
      "            # theano.config.floatX so that the code is runable on GPU\n",
      "            initial_W = numpy.asarray(\n",
      "                numpy_rng.uniform(\n",
      "                    low=-4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                    high=4 * numpy.sqrt(6. / (n_hidden + n_visible)),\n",
      "                    size=(n_visible, n_hidden)\n",
      "                ),\n",
      "                dtype=theano.config.floatX\n",
      "            )\n",
      "            W = theano.shared(value=initial_W, name='W', borrow=True)\n",
      "\n",
      "        if not bvis:\n",
      "            bvis = theano.shared(\n",
      "                value=numpy.zeros(\n",
      "                    n_visible,\n",
      "                    dtype=theano.config.floatX\n",
      "                ),\n",
      "                borrow=True\n",
      "            )\n",
      "\n",
      "        if not bhid:\n",
      "            bhid = theano.shared(\n",
      "                value=numpy.zeros(\n",
      "                    n_hidden,\n",
      "                    dtype=theano.config.floatX\n",
      "                ),\n",
      "                name='b',\n",
      "                borrow=True\n",
      "            )\n",
      "\n",
      "        self.W = W\n",
      "        # b corresponds to the bias of the hidden\n",
      "        self.b = bhid\n",
      "        # b_prime corresponds to the bias of the visible\n",
      "        self.b_prime = bvis\n",
      "        # tied weights, therefore W_prime is W transpose\n",
      "        self.W_prime = self.W.T\n",
      "        self.theano_rng = theano_rng\n",
      "        # if no input is given, generate a variable representing the input\n",
      "        if input is None:\n",
      "            # we use a matrix because we expect a minibatch of several\n",
      "            # examples, each example being a row\n",
      "            self.x = T.dmatrix(name='input')\n",
      "        else:\n",
      "            self.x = input\n",
      "\n",
      "        self.params = [self.W, self.b, self.b_prime]\n",
      "\n",
      "    def get_corrupted_input(self, input, corruption_level):\n",
      "        \"\"\"This function keeps ``1-corruption_level`` entries of the inputs the\n",
      "        same and zero-out randomly selected subset of size ``coruption_level``\n",
      "        Note : first argument of theano.rng.binomial is the shape(size) of\n",
      "               random numbers that it should produce\n",
      "               second argument is the number of trials\n",
      "               third argument is the probability of success of any trial\n",
      "\n",
      "                this will produce an array of 0s and 1s where 1 has a\n",
      "                probability of 1 - ``corruption_level`` and 0 with\n",
      "                ``corruption_level``\n",
      "\n",
      "                The binomial function return int64 data type by\n",
      "                default.  int64 multiplicated by the input\n",
      "                type(floatX) always return float64.  To keep all data\n",
      "                in floatX when floatX is float32, we set the dtype of\n",
      "                the binomial to floatX. As in our case the value of\n",
      "                the binomial is always 0 or 1, this don't change the\n",
      "                result. This is needed to allow the gpu to work\n",
      "                correctly as it only support float32 for now.\n",
      "\n",
      "        \"\"\"\n",
      "        return self.theano_rng.binomial(size=input.shape, n=1,\n",
      "                                        p=1 - corruption_level,\n",
      "                                        dtype=theano.config.floatX) * input\n",
      "\n",
      "    def get_hidden_values(self, input):\n",
      "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
      "        return T.nnet.sigmoid(T.dot(input, self.W) + self.b)\n",
      "\n",
      "    def get_reconstructed_input(self, hidden):\n",
      "        \"\"\"Computes the reconstructed input given the values of the\n",
      "        hidden layer\n",
      "\n",
      "        \"\"\"\n",
      "        return T.nnet.sigmoid(T.dot(hidden, self.W_prime) + self.b_prime)\n",
      "\n",
      "    def get_cost_updates(self, corruption_level, learning_rate):\n",
      "        \"\"\" This function computes the cost and the updates for one trainng\n",
      "        step of the dA \"\"\"\n",
      "\n",
      "        tilde_x = self.get_corrupted_input(self.x, corruption_level)\n",
      "        y = self.get_hidden_values(tilde_x)\n",
      "        z = self.get_reconstructed_input(y)\n",
      "        # note : we sum over the size of a datapoint; if we are using\n",
      "        #        minibatches, L will be a vector, with one entry per\n",
      "        #        example in minibatch\n",
      "        L = - T.sum(self.x * T.log(z) + (1 - self.x) * T.log(1 - z), axis=1)\n",
      "        # note : L is now a vector, where each element is the\n",
      "        #        cross-entropy cost of the reconstruction of the\n",
      "        #        corresponding example of the minibatch. We need to\n",
      "        #        compute the average of all these to get the cost of\n",
      "        #        the minibatch\n",
      "        cost = T.mean(L)\n",
      "\n",
      "        # compute the gradients of the cost of the `dA` with respect\n",
      "        # to its parameters\n",
      "        gparams = T.grad(cost, self.params)\n",
      "        # generate the list of updates\n",
      "        updates = [\n",
      "            (param, param - learning_rate * gparam)\n",
      "            for param, gparam in zip(self.params, gparams)\n",
      "        ]\n",
      "\n",
      "        return (cost, updates)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is easy now to construct an instance of our dA class and train it."
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "import numpy\n",
      "\n",
      "import theano\n",
      "import theano.tensor as T"
     ]
    },
    {
     "cell_type": "raw",
     "metadata": {},
     "source": [
      "  # allocate symbolic variables for the data\n",
      "index = T.lscalar()    # index to a [mini]batch   \n",
      "x = T.matrix('x')  # the data is presented as rasterized images\n",
      "\n",
      "  #####################################\n",
      "  # BUILDING THE MODEL CORRUPTION 30% #\n",
      "  #####################################\n",
      "\n",
      "rng = numpy.random.RandomState(123)\n",
      "theano_rng = theano.tensor.shared_randomstreams.RandomStreams(rng.randint(2 ** 30))\n",
      "\n",
      "da = dA(\n",
      "    numpy_rng=rng,\n",
      "    theano_rng=theano_rng,\n",
      "    input=x,\n",
      "    n_visible=28 * 28,\n",
      "    n_hidden=500\n",
      ")\n",
      "\n",
      "cost, updates = da.get_cost_updates(\n",
      "    corruption_level=0.3,\n",
      "    learning_rate=learning_rate\n",
      ")\n",
      "\n",
      "train_da = theano.function(\n",
      "    [index],\n",
      "    cost,\n",
      "    updates=updates,\n",
      "    givens={\n",
      "        x: train_set_x[index * batch_size: (index + 1) * batch_size]\n",
      "    }\n",
      ")\n",
      "\n",
      "start_time = timeit.default_timer()\n",
      "\n",
      "  ############\n",
      "  # TRAINING #\n",
      "  ############\n",
      "\n",
      "# go through training epochs\n",
      "for epoch in range(training_epochs):\n",
      "    # go through trainng set\n",
      "    c = []\n",
      "    for batch_index in range(n_train_batches):\n",
      "        c.append(train_da(batch_index))\n",
      "\n",
      "    print('Training epoch %d, cost ' % epoch, numpy.mean(c))\n",
      "\n",
      "end_time = timeit.default_timer()\n",
      "\n",
      "training_time = (end_time - start_time)\n",
      "\n",
      "print('The 30% corruption code ran for %.2fm' % (training_time / 60.))"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## 4. The Introduction of Stacked Denoising Autoencoders\n",
      "\n",
      "Denoising autoencoders can be stacked to form a deep network by feeding the latent representation (output code) of the denoising autoencoder found on the layer below as input to the current layer. \n",
      "\n",
      "The **unsupervised pre-training** of such an architecture is done one layer at a time.\n",
      "\n",
      "Once all layers are pre-trained, the network goes through a second stage of training called **fine-tuning**. \n",
      "\n",
      "Here we consider **supervised fine-tuning** where we want to minimize prediction error on a supervised task. \n",
      "\n",
      "For this, we first add a logistic regression layer on top of the network (more precisely on the output code of the output layer). \n",
      "\n",
      "We then train the entire network as we would train a multilayer perceptron. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class SdA(object):\n",
      "    \"\"\"Stacked denoising auto-encoder class (SdA)\n",
      "\n",
      "    A stacked denoising autoencoder model is obtained by stacking several\n",
      "    dAs. The hidden layer of the dA at layer `i` becomes the input of\n",
      "    the dA at layer `i+1`. The first layer dA gets as input the input of\n",
      "    the SdA, and the hidden layer of the last dA represents the output.\n",
      "    Note that after pretraining, the SdA is dealt with as a normal MLP,\n",
      "    the dAs are only used to initialize the weights.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        numpy_rng,\n",
      "        theano_rng=None,\n",
      "        n_ins=784,\n",
      "        hidden_layers_sizes=[500, 500],\n",
      "        n_outs=10,\n",
      "        corruption_levels=[0.1, 0.1]\n",
      "    ):\n",
      "        \"\"\" This class is made to support a variable number of layers.\n",
      "\n",
      "        :type numpy_rng: numpy.random.RandomState\n",
      "        :param numpy_rng: numpy random number generator used to draw initial\n",
      "                    weights\n",
      "\n",
      "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
      "        :param theano_rng: Theano random generator; if None is given one is\n",
      "                           generated based on a seed drawn from `rng`\n",
      "\n",
      "        :type n_ins: int\n",
      "        :param n_ins: dimension of the input to the sdA\n",
      "\n",
      "        :type hidden_layers_sizes: list of ints\n",
      "        :param hidden_layers_sizes: intermediate layers size, must contain\n",
      "                               at least one value\n",
      "\n",
      "        :type n_outs: int\n",
      "        :param n_outs: dimension of the output of the network\n",
      "\n",
      "        :type corruption_levels: list of float\n",
      "        :param corruption_levels: amount of corruption to use for each\n",
      "                                  layer\n",
      "        \"\"\"\n",
      "\n",
      "        self.sigmoid_layers = []\n",
      "        self.dA_layers = []\n",
      "        self.params = []\n",
      "        self.n_layers = len(hidden_layers_sizes)\n",
      "\n",
      "        assert self.n_layers > 0\n",
      "\n",
      "        if not theano_rng:\n",
      "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
      "        # allocate symbolic variables for the data\n",
      "        self.x = T.matrix('x')  # the data is presented as rasterized images\n",
      "        self.y = T.ivector('y')  # the labels are presented as 1D vector of\n",
      "                                 # [int] labels\n",
      "\n",
      "        for i in range(self.n_layers):\n",
      "            # construct the sigmoidal layer\n",
      "\n",
      "            # the size of the input is either the number of hidden units of\n",
      "            # the layer below or the input size if we are on the first layer\n",
      "            if i == 0:\n",
      "                input_size = n_ins\n",
      "            else:\n",
      "                input_size = hidden_layers_sizes[i - 1]\n",
      "\n",
      "            # the input to this layer is either the activation of the hidden\n",
      "            # layer below or the input of the SdA if you are on the first\n",
      "            # layer\n",
      "            if i == 0:\n",
      "                layer_input = self.x\n",
      "            else:\n",
      "                layer_input = self.sigmoid_layers[-1].output\n",
      "\n",
      "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
      "                                        input=layer_input,\n",
      "                                        n_in=input_size,\n",
      "                                        n_out=hidden_layers_sizes[i],\n",
      "                                        activation=T.nnet.sigmoid)\n",
      "            # add the layer to our list of layers\n",
      "            self.sigmoid_layers.append(sigmoid_layer)\n",
      "            # its arguably a philosophical question...\n",
      "            # but we are going to only declare that the parameters of the\n",
      "            # sigmoid_layers are parameters of the StackedDAA\n",
      "            # the visible biases in the dA are parameters of those\n",
      "            # dA, but not the SdA\n",
      "            self.params.extend(sigmoid_layer.params)\n",
      "\n",
      "            # Construct a denoising autoencoder that shared weights with this\n",
      "            # layer\n",
      "            dA_layer = dA(numpy_rng=numpy_rng,\n",
      "                          theano_rng=theano_rng,\n",
      "                          input=layer_input,\n",
      "                          n_visible=input_size,\n",
      "                          n_hidden=hidden_layers_sizes[i],\n",
      "                          W=sigmoid_layer.W,\n",
      "                          bhid=sigmoid_layer.b)\n",
      "            self.dA_layers.append(dA_layer)\n",
      "            \n",
      "        # We now need to add a logistic layer on top of the MLP\n",
      "        self.logLayer = LogisticRegression(\n",
      "            input=self.sigmoid_layers[-1].output,\n",
      "            n_in=hidden_layers_sizes[-1],\n",
      "            n_out=n_outs\n",
      "        )\n",
      "\n",
      "        self.params.extend(self.logLayer.params)\n",
      "        # construct a function that implements one step of finetunining\n",
      "\n",
      "        # compute the cost for second phase of training,\n",
      "        # defined as the negative log likelihood\n",
      "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
      "        # compute the gradients with respect to the model parameters\n",
      "        # symbolic variable that points to the number of errors made on the\n",
      "        # minibatch given by self.x and self.y\n",
      "        self.errors = self.logLayer.errors(self.y)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Reference:\n",
      "\n",
      "- http://deeplearning.net/tutorial/dA.html#daa\n",
      "- http://deeplearning.net/tutorial/DBN.html#dbn\n",
      "- http://deeplearning.net/tutorial/rbm.html#rbm\n",
      "- https://github.com/echen/restricted-boltzmann-machines\n",
      "- http://imonad.com/rbm/restricted-boltzmann-machine/"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}