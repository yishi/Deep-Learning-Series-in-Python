{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Document Classification Using 20 Newsgroups Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is a collection of 18,846 messages, collected from 20 different netnews newsgroups.\n",
    "You can get the data from [here](http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html).\n",
    "\n",
    "First of all, we load the data use the function from sklearn package, then use several methods to get the features, then use kinds of traditional machine learning models to fit the data.\n",
    "\n",
    "Secondly, we use deep learning algorithm in keras to train the model, because of the limited computer resources, I can not run the whole process.\n",
    "\n",
    "Below code is come from:\n",
    "\n",
    "http://blog.csdn.net/abcjennifer/article/details/23615947/\n",
    "\n",
    "http://keras-cn.readthedocs.io/en/latest/blog/word_embedding/\n",
    "\n",
    "http://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\n",
    "\n",
    "#### 1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#all categories  \n",
    "newsgroup_train = fetch_20newsgroups(subset='train')  \n",
    "newsgroup_test = fetch_20newsgroups(subset='test') \n",
    "#part categories  \n",
    "#categories = ['comp.graphics',  \n",
    "# 'comp.os.ms-windows.misc',  \n",
    "# 'comp.sys.ibm.pc.hardware',  \n",
    "# 'comp.sys.mac.hardware',  \n",
    "# 'comp.windows.x'];  \n",
    "#newsgroup_train = fetch_20newsgroups(subset = 'train',categories = categories);  \n",
    "#newsgroup_test = fetch_20newsgroups(subset = 'test',categories = categories);  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7532"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroup_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroup_train.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2 Get Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 2.1 HashingVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of fea_train:(11314, 10000)\n",
      "Size of fea_train:(7532, 10000)\n",
      "The average feature sparsity is 1.128%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer  \n",
    "\n",
    "vectorizer = HashingVectorizer(stop_words = 'english',non_negative = True,  \n",
    "                               n_features = 10000)  \n",
    "fea_train = vectorizer.fit_transform(newsgroup_train.data)  \n",
    "fea_test = vectorizer.fit_transform(newsgroup_test.data);  \n",
    "\n",
    "#return feature vector 'fea_train' [n_samples,n_features]  \n",
    "print 'Size of fea_train:' + repr(fea_train.shape)  \n",
    "print 'Size of fea_train:' + repr(fea_test.shape)  \n",
    "#11314 documents, 130107 vectors for all categories  \n",
    "print 'The average feature sparsity is {0:.3f}%'.format(  \n",
    "fea_train.nnz/float(fea_train.shape[0]*fea_train.shape[1])*100);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 CountVectorizer + TfidfTransformer\n",
    "\n",
    "Two CountVectorizer in train and test will have different feature dimentions, which will cause some trouble in below steps, so we could solve this problem by let two CountVectorizer in train and test share the same vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of train is (11314, 129792)\n",
      "the shape of test is (7532, 93251)\n"
     ]
    }
   ],
   "source": [
    "count_v1= CountVectorizer(stop_words = 'english', max_df = 0.5);  \n",
    "counts_train = count_v1.fit_transform(newsgroup_train.data);  \n",
    "print \"the shape of train is \"+repr(counts_train.shape)  \n",
    "  \n",
    "count_v2 = CountVectorizer(stop_words = 'english', max_df = 0.5);  \n",
    "counts_test = count_v2.fit_transform(newsgroup_test.data);  \n",
    "print \"the shape of test is \"+repr(counts_test.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "CountVectorizer+TfidfTransformer\n",
      "*************************\n",
      "the shape of train is (11314, 129792)\n",
      "the shape of test is (7532, 129792)\n"
     ]
    }
   ],
   "source": [
    "#method 1:CountVectorizer+TfidfTransformer  \n",
    "print '*************************\\nCountVectorizer+TfidfTransformer\\n*************************'  \n",
    "\n",
    "count_v1= CountVectorizer(stop_words = 'english', max_df = 0.5);  \n",
    "counts_train = count_v1.fit_transform(newsgroup_train.data);  \n",
    "print \"the shape of train is \"+repr(counts_train.shape)  \n",
    "  \n",
    "count_v2 = CountVectorizer(vocabulary=count_v1.vocabulary_);  \n",
    "counts_test = count_v2.fit_transform(newsgroup_test.data);  \n",
    "print \"the shape of test is \"+repr(counts_test.shape)  \n",
    "  \n",
    "tfidftransformer = TfidfTransformer();  \n",
    "  \n",
    "tfidf_train = tfidftransformer.fit(counts_train).transform(counts_train);  \n",
    "tfidf_test = tfidftransformer.fit(counts_test).transform(counts_test);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 TfidfVectorizer\n",
    "\n",
    "Let two TfidfVectorizer in train and test share the same vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "TfidfVectorizer\n",
      "*************************\n",
      "the shape of train is (11314, 129792)\n",
      "the shape of test is (7532, 129792)\n"
     ]
    }
   ],
   "source": [
    "#method 2:TfidfVectorizer  \n",
    "print '*************************\\nTfidfVectorizer\\n*************************'  \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "tv = TfidfVectorizer(sublinear_tf = True,  \n",
    "                                    max_df = 0.5,  \n",
    "                                    stop_words = 'english');  \n",
    "tfidf_train_2 = tv.fit_transform(newsgroup_train.data); \n",
    "\n",
    "tv2 = TfidfVectorizer(vocabulary = tv.vocabulary_);  \n",
    "tfidf_test_2 = tv2.fit_transform(newsgroup_test.data);  \n",
    "\n",
    "print \"the shape of train is \"+repr(tfidf_train_2.shape)  \n",
    "print \"the shape of test is \"+repr(tfidf_test_2.shape)  \n",
    "#analyze = tv.build_analyzer()  \n",
    "#tv.get_feature_names()#statistical features/terms  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 Traditional Classification Models\n",
    "\n",
    "#### 3.1 Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_result(actual,pred):  \n",
    "    m_precision = metrics.precision_score(actual,pred);  \n",
    "    m_recall = metrics.recall_score(actual,pred);  \n",
    "    print 'predict info:'  \n",
    "    print 'precision:{0:.3f}'.format(m_precision)  \n",
    "    print 'recall:{0:0.3f}'.format(m_recall);  \n",
    "    print 'f1-score:{0:.3f}'.format(metrics.f1_score(actual,pred));  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "Naive Bayes\n",
      "*************************\n",
      "********** HashingVectorizer ***************\n",
      "predict info:\n",
      "precision:0.801\n",
      "recall:0.798\n",
      "f1-score:0.796\n",
      "********** CountVectorizer + TfidfTransformer ***************\n",
      "predict info:\n",
      "precision:0.833\n",
      "recall:0.832\n",
      "f1-score:0.831\n",
      "********** TfidfVectorizer ***************\n",
      "predict info:\n",
      "precision:0.834\n",
      "recall:0.833\n",
      "f1-score:0.831\n",
      "********** fetch_20newsgroups_vectorized ***************\n",
      "predict info:\n",
      "precision:0.047\n",
      "recall:0.045\n",
      "f1-score:0.043\n"
     ]
    }
   ],
   "source": [
    "#Multinomial Naive Bayes Classifier  \n",
    "print '*************************\\nNaive Bayes\\n*************************'  \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB   \n",
    "from sklearn import metrics  \n",
    "\n",
    "#create the Multinomial Naive Bayesian Classifier  \n",
    "clf = MultinomialNB(alpha = 0.01)   \n",
    "\n",
    "print '********** HashingVectorizer ***************'\n",
    "clf.fit(fea_train, newsgroup_train.target);  \n",
    "pred = clf.predict(fea_test);  \n",
    "calculate_result(newsgroup_test.target, pred);  \n",
    "#notice here we can see that f1_score is not equal to 2*precision*recall/(precision+recall)  \n",
    "#because the m_precision and m_recall we get is averaged, however, metrics.f1_score() calculates  \n",
    "#not average, i.e., takes into the number of each class into consideration.  \n",
    "\n",
    "print '********** CountVectorizer + TfidfTransformer ***************'\n",
    "clf.fit(tfidf_train, newsgroup_train.target);  \n",
    "pred = clf.predict(tfidf_test);  \n",
    "calculate_result(newsgroup_test.target, pred);  \n",
    "\n",
    "print '********** TfidfVectorizer ***************'\n",
    "clf.fit(tfidf_train_2, newsgroup_train.target);  \n",
    "pred = clf.predict(tfidf_test_2 );  \n",
    "calculate_result(newsgroup_test.target, pred);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 K Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "KNN\n",
      "*************************\n",
      "********** HashingVectorizer ***************\n",
      "predict info:\n",
      "precision:0.629\n",
      "recall:0.616\n",
      "f1-score:0.617\n",
      "********** CountVectorizer + TfidfTransformer ***************\n",
      "predict info:\n",
      "precision:0.687\n",
      "recall:0.678\n",
      "f1-score:0.679\n",
      "********** TfidfVectorizer ***************\n",
      "predict info:\n",
      "precision:0.697\n",
      "recall:0.687\n",
      "f1-score:0.689\n",
      "********** fetch_20newsgroups_vectorized ***************\n",
      "predict info:\n",
      "precision:0.046\n",
      "recall:0.047\n",
      "f1-score:0.040\n"
     ]
    }
   ],
   "source": [
    "#KNN Classifier  \n",
    "from sklearn.neighbors import KNeighborsClassifier  \n",
    "\n",
    "print '*************************\\nKNN\\n*************************'  \n",
    "knnclf = KNeighborsClassifier()#default with k=5  \n",
    "\n",
    "print '********** HashingVectorizer ***************'\n",
    "knnclf.fit(fea_train,newsgroup_train.target)  \n",
    "pred = knnclf.predict(fea_test);  \n",
    "calculate_result(newsgroup_test.target,pred);  \n",
    "\n",
    "print '********** CountVectorizer + TfidfTransformer ***************'\n",
    "knnclf.fit(tfidf_train, newsgroup_train.target);  \n",
    "pred = knnclf.predict(tfidf_test);  \n",
    "calculate_result(newsgroup_test.target, pred);  \n",
    "\n",
    "print '********** TfidfVectorizer ***************'\n",
    "knnclf.fit(tfidf_train_2, newsgroup_train.target);  \n",
    "pred = knnclf.predict(tfidf_test_2 );  \n",
    "calculate_result(newsgroup_test.target, pred);  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Support Vector Machine (SVM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "SVM\n",
      "*************************\n",
      "********** HashingVectorizer ***************\n",
      "predict info:\n",
      "precision:0.790\n",
      "recall:0.785\n",
      "f1-score:0.784\n",
      "********** CountVectorizer + TfidfTransformer ***************\n",
      "predict info:\n",
      "precision:0.840\n",
      "recall:0.835\n",
      "f1-score:0.835\n",
      "********** TfidfVectorizer ***************\n",
      "predict info:\n",
      "precision:0.841\n",
      "recall:0.837\n",
      "f1-score:0.837\n",
      "********** fetch_20newsgroups_vectorized ***************\n",
      "predict info:\n",
      "precision:0.045\n",
      "recall:0.049\n",
      "f1-score:0.044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\application\\Anaconda_64\\lib\\site-packages\\sklearn\\metrics\\metrics.py:1771: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "D:\\application\\Anaconda_64\\lib\\site-packages\\sklearn\\metrics\\metrics.py:1771: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#SVM Classifier  \n",
    "from sklearn.svm import SVC  \n",
    "\n",
    "print '*************************\\nSVM\\n*************************'  \n",
    "svclf = SVC(kernel = 'linear')#default with 'rbf'  \n",
    "\n",
    "print '********** HashingVectorizer ***************'\n",
    "svclf.fit(fea_train,newsgroup_train.target)  \n",
    "pred = svclf.predict(fea_test);  \n",
    "calculate_result(newsgroup_test.target,pred); \n",
    "\n",
    "print '********** CountVectorizer + TfidfTransformer ***************'\n",
    "svclf.fit(tfidf_train, newsgroup_train.target);  \n",
    "pred = svclf.predict(tfidf_test);  \n",
    "calculate_result(newsgroup_test.target, pred);  \n",
    "\n",
    "print '********** TfidfVectorizer ***************'\n",
    "svclf.fit(tfidf_train_2, newsgroup_train.target);  \n",
    "pred = svclf.predict(tfidf_test_2 );  \n",
    "calculate_result(newsgroup_test.target, pred);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.4 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "Logistic Regression\n",
      "*************************\n",
      "********** HashingVectorizer ***************\n",
      "predict info:\n",
      "precision:0.774\n",
      "recall:0.772\n",
      "f1-score:0.769\n",
      "********** CountVectorizer + TfidfTransformer ***************\n",
      "predict info:\n",
      "precision:0.833\n",
      "recall:0.830\n",
      "f1-score:0.828\n",
      "********** TfidfVectorizer ***************\n",
      "predict info:\n",
      "precision:0.835\n",
      "recall:0.832\n",
      "f1-score:0.830\n"
     ]
    }
   ],
   "source": [
    "#Logistic Regression Classifier  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "print '*************************\\nLogistic Regression\\n*************************'  \n",
    "lrclf = LogisticRegression()#default with 'ovr'  \n",
    "\n",
    "print '********** HashingVectorizer ***************'\n",
    "lrclf.fit(fea_train,newsgroup_train.target)  \n",
    "pred = lrclf.predict(fea_test);  \n",
    "calculate_result(newsgroup_test.target,pred); \n",
    "\n",
    "print '********** CountVectorizer + TfidfTransformer ***************'\n",
    "lrclf.fit(tfidf_train, newsgroup_train.target);  \n",
    "pred = lrclf.predict(tfidf_test);  \n",
    "calculate_result(newsgroup_test.target, pred);  \n",
    "\n",
    "print '********** TfidfVectorizer ***************'\n",
    "lrclf.fit(tfidf_train_2, newsgroup_train.target);  \n",
    "pred = lrclf.predict(tfidf_test_2 );  \n",
    "calculate_result(newsgroup_test.target, pred);   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 4 Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BASE_DIR = 'scikit_learn_data'\n",
    "GLOVE_DIR = BASE_DIR + '/glove.6B/'\n",
    "TEXT_DATA_DIR = BASE_DIR + '/20news-bydate/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11314 texts in train.\n"
     ]
    }
   ],
   "source": [
    "texts_train = []  # list of text samples\n",
    "labels_index_train = {}  # dictionary mapping label name to numeric id\n",
    "labels_train = []  # list of label ids\n",
    "train_data_dir = TEXT_DATA_DIR + '20news-bydate-train/'\n",
    "\n",
    "for name in sorted(os.listdir(train_data_dir)):\n",
    "    path = os.path.join(train_data_dir, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index_train)\n",
    "        labels_index_train[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath)\n",
    "                texts_train.append(f.read())\n",
    "                f.close()\n",
    "                labels_train.append(label_id)\n",
    "\n",
    "print('Found %s texts in train.' % len(texts_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7532 texts in test.\n"
     ]
    }
   ],
   "source": [
    "texts_test = []  # list of text samples\n",
    "labels_index_test = {}  # dictionary mapping label name to numeric id\n",
    "labels_test = []  # list of label ids\n",
    "test_data_dir = TEXT_DATA_DIR + '20news-bydate-test/'\n",
    "\n",
    "for name in sorted(os.listdir(test_data_dir)):\n",
    "    path = os.path.join(test_data_dir, name)\n",
    "    if os.path.isdir(path):\n",
    "        label_id = len(labels_index_test)\n",
    "        labels_index_test[name] = label_id\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                f = open(fpath)\n",
    "                texts_test.append(f.read())\n",
    "                f.close()\n",
    "                labels_test.append(label_id)\n",
    "\n",
    "print('Found %s texts in test.' % len(texts_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 179198 unique tokens.\n",
      "('Shape of x_train tensor:', (11314L, 1000L))\n",
      "('Shape of labels_train tensor:', (11314L, 20L))\n",
      "('Shape of x_test tensor:', (7532L, 1000L))\n",
      "('Shape of labels_test tensor:', (7532L, 20L))\n"
     ]
    }
   ],
   "source": [
    "texts = texts_train + texts_test\n",
    "\n",
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "x_train = data[:len(texts_train)]\n",
    "x_test = data[len(texts_train):]\n",
    "labels_train = to_categorical(np.asarray(labels_train))\n",
    "labels_test = to_categorical(np.asarray(labels_test))\n",
    "\n",
    "print('Shape of x_train tensor:', x_train.shape)\n",
    "print('Shape of labels_train tensor:', labels_train.shape)\n",
    "print('Shape of x_test tensor:', x_test.shape)\n",
    "print('Shape of labels_test tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Initializing our Embedding layer from scratch and learning its weights during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named mkl\n",
      "WARNING:theano.configdefaults:install mkl with `conda install mkl-service`: No module named mkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/10\n",
      "11264/11314 [============================>.] - ETA: 15s - loss: 2.7359 - acc: 0.1090Traceback (most recent call last):\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 233, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 267, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\inspect.py\", line 1044, in getinnerframes\n",
      "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\inspect.py\", line 1004, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\inspect.py\", line 454, in getsourcefile\n",
      "    if hasattr(getmodule(object, filename), '__loader__'):\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\inspect.py\", line 497, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\inspect.py\", line 466, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\inspect.py\", line 448, in getsourcefile\n",
      "    if 'b' in mode and string.lower(filename[-len(suffix):]) == suffix:\n",
      "  File \"D:\\application\\Anaconda_64\\lib\\string.py\", line 220, in lower\n",
      "    return s.lower()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mD:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_code\u001b[1;34m(self, code_obj, result)\u001b[0m\n\u001b[0;32m   3047\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3048\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_in_exec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3049\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3051\u001b[0m             \u001b[0moutflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\interactiveshell.pyc\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only)\u001b[0m\n\u001b[0;32m   1846\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[1;32m-> 1848\u001b[1;33m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[1;32m-> 1240\u001b[1;33m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[1;32m-> 1148\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m             )\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.pyc\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[1;32m-> 1000\u001b[1;33m                                                                tb_offset)\n\u001b[0m\u001b[0;32m   1001\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[1;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.pyc\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    949\u001b[0m         \u001b[0mrecords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrecords\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\application\\Anaconda_64\\lib\\site-packages\\IPython\\core\\ultratb.pyc\u001b[0m in \u001b[0;36mformat_records\u001b[1;34m(self, records)\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m         \u001b[0mabspath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 724\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    725\u001b[0m             \u001b[1;31m#print '*** record:',file,lnum,func,lines,index  # dbg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index_train), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, labels_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 GloVe word embeddings\n",
    "\n",
    "- GloVe stands for \"Global Vectors for Word Representation\". \n",
    "- It's a somewhat popular embedding technique based on factorizing a matrix of word co-occurence statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer_glove = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named mkl\n",
      "WARNING:theano.configdefaults:install mkl with `conda install mkl-service`: No module named mkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/2\n",
      "  384/11314 [>.............................] - ETA: 2078s - loss: 2.9939 - acc: 0.0651"
     ]
    }
   ],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer_glove(sequence_input)\n",
    "\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(len(labels_index_train), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, labels_train,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          validation_data=(x_test, labels_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
